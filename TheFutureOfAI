{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dascient/the-future-of-ai?scriptVersionId=223333922\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Data Exploration and Analysis with Python (with GPU Acceleration)\n\nThis notebook guides you through data analysis and modeling using Python. It includes:\n- Data loading and preprocessing\n- Exploratory data analysis (EDA)\n- Visualization techniques\n- Basic machine learning with scikit-learn\n- GPU-accelerated modeling with RAPIDS (cuDF and cuML)\n\nEach section contains detailed explanations and code comments to aid your learning process. Further resources are provided at the end.","metadata":{}},{"cell_type":"markdown","source":"## 1. Environment Setup\n\nWe begin by importing the necessary libraries. For general data manipulation, visualization, and CPU-based modeling we use libraries like `pandas`, `numpy`, `matplotlib`, `seaborn`, and `scikit-learn`. Later, we introduce RAPIDS libraries (`cudf` and `cuML`) to take advantage of GPU acceleration.","metadata":{}},{"cell_type":"code","source":"# Import necessary CPU-based libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For CPU-based machine learning\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Configure visualization styles\nsns.set(style=\"whitegrid\")\n%matplotlib inline\n\n# Display settings for pandas\npd.set_option('display.max_columns', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:41:09.376268Z","iopub.execute_input":"2025-02-19T07:41:09.376448Z","iopub.status.idle":"2025-02-19T07:41:12.89686Z","shell.execute_reply.started":"2025-02-19T07:41:09.376429Z","shell.execute_reply":"2025-02-19T07:41:12.895833Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Data Loading and Preprocessing\n\nWe load the Iris dataset from scikit-learn—a well-known dataset in pattern recognition. We inspect the structure and verify that there are no missing values.","metadata":{}},{"cell_type":"code","source":"# Load the Iris dataset\niris = load_iris()\n# Create a DataFrame with feature data and target labels\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\ndf['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n# Display the first few rows\nprint(\"Dataset preview:\")\ndisplay(df.head())\n\n# Check for missing values\nprint(\"Missing values per column:\")\ndisplay(df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:41:12.898243Z","iopub.execute_input":"2025-02-19T07:41:12.898685Z","iopub.status.idle":"2025-02-19T07:41:12.943896Z","shell.execute_reply.started":"2025-02-19T07:41:12.898657Z","shell.execute_reply":"2025-02-19T07:41:12.943042Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Exploratory Data Analysis (EDA)\n\nBefore modeling, it is essential to understand the dataset. We first compute summary statistics and then visualize the distributions and relationships between features.","metadata":{}},{"cell_type":"code","source":"# Summary statistics\nprint(\"Summary statistics:\")\ndisplay(df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:41:12.945052Z","iopub.execute_input":"2025-02-19T07:41:12.945274Z","iopub.status.idle":"2025-02-19T07:41:12.977764Z","shell.execute_reply.started":"2025-02-19T07:41:12.945254Z","shell.execute_reply":"2025-02-19T07:41:12.976825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.1 Visualizing Feature Distributions\n\nHistograms for each feature help reveal the range and distribution of the measurements.","metadata":{}},{"cell_type":"code","source":"# Plot histograms for each feature\ndf.hist(bins=20, figsize=(12, 8))\nplt.suptitle(\"Histograms of Iris Features\", fontsize=16)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:41:12.978784Z","iopub.execute_input":"2025-02-19T07:41:12.97919Z","iopub.status.idle":"2025-02-19T07:41:14.656509Z","shell.execute_reply.started":"2025-02-19T07:41:12.979155Z","shell.execute_reply":"2025-02-19T07:41:14.655587Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2 Pairwise Relationships\n\nPair plots illustrate the relationships between pairs of features, with points colored by species. This visualization can hint at how well the classes are separated.","metadata":{}},{"cell_type":"code","source":"# Create a pair plot with seaborn\nsns.pairplot(df, hue=\"species\", markers=[\"o\", \"s\", \"D\"])\nplt.suptitle(\"Pairwise Relationships in Iris Dataset\", fontsize=16)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:41:14.658499Z","iopub.execute_input":"2025-02-19T07:41:14.658744Z","iopub.status.idle":"2025-02-19T07:41:19.762541Z","shell.execute_reply.started":"2025-02-19T07:41:14.658703Z","shell.execute_reply":"2025-02-19T07:41:19.761436Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Basic Modeling with scikit-learn\n\nWe perform a classification task using a Random Forest classifier. The steps include:\n- Splitting the data into training and test sets\n- Training the model\n- Evaluating model performance\n\nThis section illustrates a typical machine learning workflow on the CPU.","metadata":{}},{"cell_type":"code","source":"# Split the dataset into features (X) and target (y)\nX = df[iris.feature_names]\ny = df['species']\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(\"Training set size:\", X_train.shape[0])\nprint(\"Test set size:\", X_test.shape[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:41:19.763753Z","iopub.execute_input":"2025-02-19T07:41:19.763993Z","iopub.status.idle":"2025-02-19T07:41:19.773102Z","shell.execute_reply.started":"2025-02-19T07:41:19.763971Z","shell.execute_reply":"2025-02-19T07:41:19.772279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize and train the Random Forest classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = clf.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"CPU Model Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:41:19.773961Z","iopub.execute_input":"2025-02-19T07:41:19.774253Z","iopub.status.idle":"2025-02-19T07:41:19.947639Z","shell.execute_reply.started":"2025-02-19T07:41:19.774223Z","shell.execute_reply":"2025-02-19T07:41:19.946805Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Accelerated Modeling with RAPIDS (GPU)\n\nNow that you've enabled Kaggle's GPU support (T4 x2), you can harness GPU acceleration using RAPIDS libraries. While our dataset is small, the following demonstrates how to use `cudf` for DataFrame operations and `cuML` for machine learning—an approach that can be significantly faster on larger datasets.\n\n### 5.1 Setting Up RAPIDS\n\nThe RAPIDS libraries (cuDF and cuML) provide GPU-accelerated versions of common data science libraries. On Kaggle, these libraries are typically pre-installed or can be installed via pip. We first import them.","metadata":{}},{"cell_type":"code","source":"try:\n    import cudf\n    from cuml.ensemble import RandomForestClassifier as cumlRandomForestClassifier\n    print(\"Successfully imported RAPIDS libraries (cuDF and cuML).\")\nexcept ImportError as e:\n    print(\"RAPIDS libraries not found. You may need to install them.\")\n    # Uncomment the following lines if installation is necessary\n    # !pip install cudf-cu11 cuml-cu11 --extra-index-url=https://pypi.ngc.nvidia.com\n    # import cudf\n    # from cuml.ensemble import RandomForestClassifier as cumlRandomForestClassifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:41:19.948417Z","iopub.execute_input":"2025-02-19T07:41:19.948633Z","iopub.status.idle":"2025-02-19T07:41:28.830222Z","shell.execute_reply.started":"2025-02-19T07:41:19.948614Z","shell.execute_reply":"2025-02-19T07:41:28.829456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.2 Converting Data to cuDF\n\nWe convert our Pandas DataFrame to a cuDF DataFrame. This allows us to leverage GPU-accelerated operations.","metadata":{}},{"cell_type":"code","source":"# Convert Pandas DataFrame to cuDF DataFrame\ngdf = cudf.DataFrame.from_pandas(df)\nprint(\"cuDF DataFrame preview:\")\ndisplay(gdf.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:41:28.831096Z","iopub.execute_input":"2025-02-19T07:41:28.831728Z","iopub.status.idle":"2025-02-19T07:41:29.252596Z","shell.execute_reply.started":"2025-02-19T07:41:28.83169Z","shell.execute_reply":"2025-02-19T07:41:29.251665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.3 GPU-Based Modeling with cuML\n\nWe train a Random Forest classifier using cuML. Note that while the API is similar to scikit-learn's, some differences may exist. Here we follow a similar train-test split procedure, converting data as needed.","metadata":{}},{"cell_type":"code","source":"# Prepare features and target using cuDF\nfeature_cols = iris.feature_names\nX_gpu = gdf[feature_cols]\ny_gpu = gdf['species']\n\n# Convert categorical target into numerical codes (matching the original iris.target codes)\n# Since cuML classifiers expect numerical targets, we map the species accordingly.\ny_gpu = y_gpu.astype('category').cat.codes\n\n# Split the GPU DataFrame into training and test sets using scikit-learn's function and then convert back to cuDF\nX_train_cpu, X_test_cpu, y_train_cpu, y_test_cpu = train_test_split(\n    X_gpu.to_pandas(), y_gpu.to_pandas(), test_size=0.3, random_state=42, stratify=y_gpu.to_pandas()\n)\n\n# Convert the split datasets back to cuDF DataFrames/Series\nX_train_gpu = cudf.DataFrame.from_pandas(X_train_cpu)\nX_test_gpu = cudf.DataFrame.from_pandas(X_test_cpu)\ny_train_gpu = cudf.Series(y_train_cpu)\ny_test_gpu = cudf.Series(y_test_cpu)\n\n# Initialize and train the GPU Random Forest classifier\ngpu_clf = cumlRandomForestClassifier(n_estimators=100, random_state=42)\ngpu_clf.fit(X_train_gpu, y_train_gpu)\n\n# Make predictions on the test set\ny_pred_gpu = gpu_clf.predict(X_test_gpu)\n\n# Evaluate the GPU model\n# Convert predictions and ground truth back to CPU (Pandas) for evaluation using scikit-learn metrics\naccuracy_gpu = accuracy_score(y_test_gpu.to_pandas(), y_pred_gpu.to_pandas())\nprint(\"GPU Model Accuracy:\", accuracy_gpu)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:41:29.253486Z","iopub.execute_input":"2025-02-19T07:41:29.253755Z","iopub.status.idle":"2025-02-19T07:41:31.896286Z","shell.execute_reply.started":"2025-02-19T07:41:29.253708Z","shell.execute_reply":"2025-02-19T07:41:31.895553Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.4 Comparing CPU and GPU Models\n\nIn this section, we display the accuracy of both models. While the Iris dataset is too small to see dramatic speed-ups, on larger datasets the GPU acceleration offered by RAPIDS can reduce training and inference times considerably.","metadata":{}},{"cell_type":"code","source":"print(\"CPU Model Accuracy:\", accuracy)\nprint(\"GPU Model Accuracy:\", accuracy_gpu)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:41:31.897058Z","iopub.execute_input":"2025-02-19T07:41:31.897288Z","iopub.status.idle":"2025-02-19T07:41:31.902599Z","shell.execute_reply.started":"2025-02-19T07:41:31.897268Z","shell.execute_reply":"2025-02-19T07:41:31.901794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Feature Importance Analysis\n\nUnderstanding which features drive the model's decisions can be insightful. We compute and display feature importances using the CPU model.","metadata":{}},{"cell_type":"code","source":"# Extract feature importances from the CPU-based Random Forest\nfeature_importances = pd.Series(clf.feature_importances_, index=iris.feature_names)\nfeature_importances = feature_importances.sort_values(ascending=False)\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x=feature_importances, y=feature_importances.index)\nplt.title(\"Feature Importance in the Iris Dataset\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:41:31.903334Z","iopub.execute_input":"2025-02-19T07:41:31.903618Z","iopub.status.idle":"2025-02-19T07:41:32.119378Z","shell.execute_reply.started":"2025-02-19T07:41:31.903595Z","shell.execute_reply":"2025-02-19T07:41:32.118614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Additional Learning Resources\n\nFor further study, consider exploring these resources:\n- **Python for Data Analysis** by Wes McKinney – A guide to effective data manipulation with Python.\n- [Pandas Documentation](https://pandas.pydata.org/docs/) – Official guide for the pandas library.\n- [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html) – In-depth resources on data visualization.\n- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html) – Comprehensive machine learning documentation.\n- [RAPIDS Documentation](https://docs.rapids.ai/) – Information on GPU-accelerated data science and machine learning.\n\nThese materials offer further insights and exercises to expand your understanding.","metadata":{}},{"cell_type":"markdown","source":"## 8. Summary and Conclusions\n\nIn this notebook, we have:\n- Loaded and inspected a classic dataset.\n- Conducted exploratory data analysis to uncover patterns.\n- Visualized feature distributions and pairwise relationships.\n- Implemented and evaluated a classification model on both CPU and GPU.\n- Demonstrated the use of RAPIDS libraries for GPU acceleration.\n\nThis notebook serves as a stepping stone for more advanced data analysis and modeling projects. By leveraging GPU acceleration, especially on larger datasets, you can achieve significant performance improvements. Enjoy your journey in data science and machine learning!","metadata":{}}]}