{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"To solve the Konwinski Prize competition on Kaggle, we need to approach the problem by developing a Python solution that automatically addresses GitHub issues using AI. This involves building a system capable of reading and understanding GitHub issues and using AI to suggest potential fixes, comments, or even create pull requests based on the issues. The challenge emphasizes the use of open-source code and open-weight models. The goal is to make it easier for human software engineers to focus on more strategic, creative tasks while automating issue resolution in the software development process.\n\nBelow is a comprehensive step-by-step Python solution template for this competition. The code outlines the major steps, including data preparation, model training, issue analysis, prediction generation, and submission creation.\n\n### Step-by-Step Python Script for GitHub Issue Automation\n\nStep 1: Setup and Install Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets scikit-learn numpy pandas matplotlib torch openai gitpython","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T09:31:28.811037Z","iopub.execute_input":"2025-01-04T09:31:28.811409Z","iopub.status.idle":"2025-01-04T09:31:36.592681Z","shell.execute_reply.started":"2025-01-04T09:31:28.811380Z","shell.execute_reply":"2025-01-04T09:31:36.591326Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nCollecting openai\n  Downloading openai-1.59.3-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (3.1.43)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\nCollecting httpx<1,>=0.23.0 (from openai)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting jiter<1,>=0.4.0 (from openai)\n  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython) (4.0.11)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\nCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nDownloading openai-1.59.3-py3-none-any.whl (454 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\nSuccessfully installed h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.2 openai-1.59.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Installations","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nimport openai\nimport git\nimport os\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T09:32:27.585138Z","iopub.execute_input":"2025-01-04T09:32:27.585512Z","iopub.status.idle":"2025-01-04T09:32:46.453237Z","shell.execute_reply.started":"2025-01-04T09:32:27.585481Z","shell.execute_reply":"2025-01-04T09:32:46.452255Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Load and Preprocess Data","metadata":{}},{"cell_type":"code","source":"# Load GitHub issues dataset or GitHub issue data from an external file\ndef load_issues_data(filepath):\n    issues_df = pd.read_csv(filepath)\n    issues_df = issues_df.dropna(subset=['issue_title', 'issue_description'])\n    return issues_df\n\n# Example: Loading issues data from a CSV file\nissues_data = load_issues_data(\"github_issues_data.csv\")\nissues_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The dataset might contain columns like:\n\n- issue_id: Unique identifier for the issue.\n- issue_title: Title of the GitHub issue.\n- issue_description: Description of the issue.\n- labels: Labels assigned to the issue (bug, enhancement, etc.).\n- issue_comments: A history of comments made on the issue.\n\n## Tokenization and Model Preparation\n\nWe'll use a pre-trained language model to classify or suggest comments on GitHub issues. In this case, we can leverage a transformer model such as BERT or GPT-3 for generating responses or code fixes for the issues.","metadata":{}},{"cell_type":"code","source":"# Load pre-trained tokenizer and model\nmodel_name = \"microsoft/deberta-v3-small\"  # You can choose a model that works best for GitHub issue categorization\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Alternatively, use a pre-trained GPT model for more advanced text generation (fixes, suggestions)\nopenai.api_key = \"your_openai_api_key\"  # To use GPT-3 or similar models\n\n# Function to generate suggestions for GitHub issues using GPT-3\ndef generate_suggestion_for_issue(issue_title, issue_description):\n    prompt = f\"Issue Title: {issue_title}\\nIssue Description: {issue_description}\\n\\nWhat is the solution or suggestion?\"\n    response = openai.Completion.create(\n        engine=\"text-davinci-003\",  # You can also use other GPT-3 models\n        prompt=prompt,\n        max_tokens=150\n    )\n    return response.choices[0].text.strip()\n\n# Example suggestion for one issue\nsample_issue_title = \"Fix bug in user authentication\"\nsample_issue_description = \"There is an issue where the login is not working when using Facebook login.\"\nsuggestion = generate_suggestion_for_issue(sample_issue_title, sample_issue_description)\nprint(\"Suggested Fix:\", suggestion)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train or Fine-Tune the Model\n\nIf you're not using a model like GPT-3 for suggestion generation but rather want to fine-tune a model for specific tasks (such as issue classification), you can proceed as follows. We’ll train a classification model based on the issue title and description to predict the category of the issue.","metadata":{}},{"cell_type":"code","source":"# Split the data into training and test sets\ntrain_data, test_data = train_test_split(issues_data, test_size=0.2)\n\n# Tokenize the text data for training\ndef tokenize_data(data, tokenizer):\n    return tokenizer(list(data['issue_title'] + \" \" + data['issue_description']), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n\ntrain_encodings = tokenize_data(train_data, tokenizer)\ntest_encodings = tokenize_data(test_data, tokenizer)\n\n# Create dataset class\nclass GitHubIssuesDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Train the model\ntrain_dataset = GitHubIssuesDataset(train_encodings, train_data['labels'].values)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n# Fine-tune the model using a training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\nfor epoch in range(3):  # Number of epochs\n    model.train()\n    for batch in train_loader:\n        optimizer.zero_grad()\n        inputs = {key: value.to(model.device) for key, value in batch.items()}\n        labels = batch['labels'].to(model.device)\n        outputs = model(**inputs)\n        loss = torch.nn.CrossEntropyLoss()(outputs.logits, labels)\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1}: Loss {loss.item()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation\n\nAfter training, we need to evaluate our model performance on the test set.","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test data\ndef evaluate_model(model, test_data, tokenizer):\n    model.eval()\n    test_encodings = tokenize_data(test_data, tokenizer)\n    test_dataset = GitHubIssuesDataset(test_encodings, test_data['labels'].values)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n    correct_predictions = 0\n    total_predictions = 0\n\n    with torch.no_grad():\n        for batch in test_loader:\n            inputs = {key: value.to(model.device) for key, value in batch.items()}\n            labels = batch['labels'].to(model.device)\n            outputs = model(**inputs)\n            predictions = torch.argmax(outputs.logits, dim=-1)\n\n            correct_predictions += (predictions == labels).sum().item()\n            total_predictions += labels.size(0)\n\n    accuracy = correct_predictions / total_predictions\n    print(f\"Model Accuracy: {accuracy:.4f}\")\n\n# Evaluate the trained model\nevaluate_model(model, test_data, tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Predictions and Submit to Kaggle\n\nAfter the model is trained and evaluated, we can generate predictions for new GitHub issues in the test set and prepare the submission to Kaggle.","metadata":{}},{"cell_type":"code","source":"# Generate predictions for the test set\ndef generate_predictions_for_submission(test_data):\n    predictions = []\n    for _, row in test_data.iterrows():\n        suggestion = generate_suggestion_for_issue(row['issue_title'], row['issue_description'])\n        predictions.append(suggestion)\n    return predictions\n\n# Generate predictions for the test set\ntest_predictions = generate_predictions_for_submission(test_data)\n\n# Prepare the Kaggle submission file\nsubmission_df = pd.DataFrame({\n    'issue_id': test_data['issue_id'],\n    'predicted_suggestion': test_predictions\n})\n\n# Save submission file\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion\nThis Python solution template demonstrates how to solve the Konwinski Prize competition challenge. The script covers:\n\n- Data loading and preprocessing: Load GitHub issue data and preprocess it.\n- Modeling: Use a transformer model for issue classification or GPT-3 for generating suggestions/fixes for GitHub issues.\n- Model training: Fine-tune a transformer model to classify issues or suggest fixes.\n- Evaluation: Evaluate the model performance on the test set.\n- Prediction generation: Use the trained model to generate predictions and suggestions for GitHub issues.\n- Submission: Prepare the Kaggle submission file.\n\nThis solution leverages open-source models, uses transformers for advanced NLP tasks, and generates practical, actionable outputs for software engineers to review and act upon.","metadata":{}},{"cell_type":"code","source":"# enfin/","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}