{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.x"}},"nbformat_minor":5,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dascient/gpu-accelerated-modeling-with-python?scriptVersionId=223331402\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"id":"6c889424-56ce-4c3d-a4a1-48a3f88a9289","cell_type":"markdown","source":"# Data Exploration, Analysis, and GPU-Accelerated Modeling with Python\n\nThis notebook provides a comprehensive guide to data analysis and modeling by combining traditional CPU-based workflows with extensive GPU-accelerated processing. We explore the Iris dataset using Pandas, scikit-learn, and Matplotlib, then leverage RAPIDS (cuDF, cuML) and CuPy to accelerate data manipulation and machine learning tasks on GPU. Additional sections demonstrate GPU profiling and performance comparisons. Detailed code comments and explanations are provided to aid your understanding throughout the process.","metadata":{}},{"id":"2c7ddafb-0f70-4d89-9f50-18f77fba1565","cell_type":"markdown","source":"## 1. Environment Setup\n\nWe begin by importing the necessary libraries. We use Pandas, NumPy, Matplotlib, Seaborn, and scikit-learn for standard data manipulation, visualization, and modeling. Later sections introduce GPU-accelerated libraries such as RAPIDS (cuDF, cuML) and CuPy to fully harness the power of the GPU available in your Kaggle environment.","metadata":{}},{"id":"55c46a47-4e4f-44ef-9d34-1b2e8c15c45f","cell_type":"code","source":"# Import CPU-based libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Configure visualization styles\nsns.set(style=\"whitegrid\")\n%matplotlib inline\n\npd.set_option('display.max_columns', None)\n\n# Try to import GPU-accelerated libraries\ntry:\n    import cudf\n    from cuml.ensemble import RandomForestClassifier as cumlRandomForestClassifier\n    print(\"RAPIDS libraries (cuDF, cuML) imported successfully.\")\nexcept ImportError as e:\n    print(\"RAPIDS libraries not found. Ensure you have the correct environment.\")\n\ntry:\n    import cupy as cp\n    print(\"CuPy imported successfully for GPU array operations.\")\nexcept ImportError as e:\n    print(\"CuPy not found. Install it for GPU-accelerated NumPy-like operations.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"b178db55-5f32-4e8f-9f44-cd811e1b89a1","cell_type":"markdown","source":"## 2. Data Loading and Preprocessing\n\nWe load the Iris dataset, a classic example in pattern recognition, and inspect its structure. The dataset is small; however, the techniques demonstrated here will scale to larger, more complex data.","metadata":{}},{"id":"1f7edcdc-c8a1-42a7-92af-44b4c3bd36c1","cell_type":"code","source":"# Load the Iris dataset\niris = load_iris()\n\n# Create a DataFrame with features and target labels\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\ndf['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n# Display a preview of the dataset\nprint(\"Dataset Preview:\")\ndisplay(df.head())\n\n# Check for missing values\nprint(\"Missing values per column:\")\ndisplay(df.isnull().sum())","metadata":{},"outputs":[],"execution_count":null},{"id":"2d95ce61-7de6-4b2f-9eea-8d5d7aa9c5f8","cell_type":"markdown","source":"## 3. Exploratory Data Analysis (EDA)\n\nBefore diving into modeling, we explore the dataset to understand its characteristics. We compute summary statistics, visualize feature distributions, and examine pairwise relationships between variables.","metadata":{}},{"id":"f65a6b14-97a9-451f-8332-ec2f51f3de3b","cell_type":"code","source":"# Display summary statistics\nprint(\"Summary Statistics:\")\ndisplay(df.describe())","metadata":{},"outputs":[],"execution_count":null},{"id":"1d414c51-9b71-4b6a-99bb-54d3a9e7f7bd","cell_type":"markdown","source":"### 3.1 Feature Distributions\n\nWe create histograms for each feature to visualize the data distribution and range of values.","metadata":{}},{"id":"416eabf8-055a-4e43-bbea-0123f9283110","cell_type":"code","source":"# Plot histograms for each feature\ndf.hist(bins=20, figsize=(12, 8))\nplt.suptitle(\"Feature Distributions in Iris Dataset\", fontsize=16)\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"21de9b84-2787-4777-99d1-c8ce84627a29","cell_type":"markdown","source":"### 3.2 Pairwise Feature Relationships\n\nA pair plot visualizes the relationships between every pair of features. The data points are colored by species to reveal class separability.","metadata":{}},{"id":"c4e67b78-9bde-42a5-bcf3-1a9f6b7fbbc3","cell_type":"code","source":"sns.pairplot(df, hue=\"species\", markers=[\"o\", \"s\", \"D\"])\nplt.suptitle(\"Pairwise Relationships in Iris Dataset\", fontsize=16)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"ba236fd9-6370-42a0-9e5e-d7f87db43d79","cell_type":"markdown","source":"## 4. CPU-Based Modeling with scikit-learn\n\nWe now split the dataset into training and testing sets and train a Random Forest classifier using scikit-learn. This demonstrates a typical machine learning workflow on CPU.","metadata":{}},{"id":"cf6f41bd-df54-4dff-a9d4-0ed2b1ef27e7","cell_type":"code","source":"# Define features (X) and target (y)\nX = df[iris.feature_names]\ny = df['species']\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(\"Training set size:\", X_train.shape[0])\nprint(\"Test set size:\", X_test.shape[0])","metadata":{},"outputs":[],"execution_count":null},{"id":"1e1bd2c9-5de1-4ea3-b14e-9d9f61a0f93a","cell_type":"code","source":"# Initialize and train the Random Forest classifier on CPU\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = clf.predict(X_test)\n\n# Evaluate the model performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"CPU Model Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{},"outputs":[],"execution_count":null},{"id":"20c0d6e3-bff9-42b7-923f-df91396cdb3b","cell_type":"markdown","source":"## 5. GPU-Accelerated Modeling with RAPIDS and CuPy\n\nNext, we convert our data to GPU-supported formats using cuDF and CuPy. We then train a Random Forest classifier using cuML. While the Iris dataset is small, these techniques are designed for large-scale datasets where GPU acceleration yields dramatic improvements.","metadata":{}},{"id":"a5f4cbb3-39a0-41bb-aab5-7cf1a7584ac1","cell_type":"markdown","source":"### 5.1 Converting Data to cuDF\n\nWe convert our Pandas DataFrame to a cuDF DataFrame to enable GPU-accelerated operations.","metadata":{}},{"id":"9ecbfc33-6f80-4a0c-ae2a-3e0c8320c8e8","cell_type":"code","source":"# Convert the Pandas DataFrame to a cuDF DataFrame\ngdf = cudf.DataFrame.from_pandas(df)\nprint(\"cuDF DataFrame Preview:\")\ndisplay(gdf.head())","metadata":{},"outputs":[],"execution_count":null},{"id":"76a61496-8b0a-4d2c-a0da-2d32a6878d6d","cell_type":"markdown","source":"### 5.2 GPU-Based Modeling with cuML\n\nWe prepare our data for GPU-based training. Note that cuML requires numeric targets, so we convert species to numerical codes. We then perform a train-test split and train a Random Forest classifier using cuML.","metadata":{}},{"id":"b0486a3b-1737-4b6a-8d5b-3a3a85c76caa","cell_type":"code","source":"# Prepare features and target for GPU processing\nfeature_cols = iris.feature_names\nX_gpu = gdf[feature_cols]\ny_gpu = gdf['species']\n\n# Convert the categorical target into numerical codes\ny_gpu = y_gpu.astype('category').cat.codes\n\n# Split the GPU DataFrame (using Pandas as intermediary) and convert back to cuDF\nX_train_cpu, X_test_cpu, y_train_cpu, y_test_cpu = train_test_split(\n    X_gpu.to_pandas(), y_gpu.to_pandas(), test_size=0.3, random_state=42, stratify=y_gpu.to_pandas()\n)\n\nX_train_gpu = cudf.DataFrame.from_pandas(X_train_cpu)\nX_test_gpu = cudf.DataFrame.from_pandas(X_test_cpu)\ny_train_gpu = cudf.Series(y_train_cpu)\ny_test_gpu = cudf.Series(y_test_cpu)\n\n# Initialize and train the GPU Random Forest classifier using cuML\ngpu_clf = cumlRandomForestClassifier(n_estimators=100, random_state=42)\ngpu_clf.fit(X_train_gpu, y_train_gpu)\n\n# Make predictions on the test set using GPU\ny_pred_gpu = gpu_clf.predict(X_test_gpu)\n\n# Evaluate GPU model (convert predictions to Pandas for scikit-learn metrics)\naccuracy_gpu = accuracy_score(y_test_gpu.to_pandas(), y_pred_gpu.to_pandas())\nprint(\"GPU Model Accuracy:\", accuracy_gpu)","metadata":{},"outputs":[],"execution_count":null},{"id":"7b1e983a-70bd-43b3-8b24-1e47d1065162","cell_type":"markdown","source":"### 5.3 GPU Performance Profiling with CuPy\n\nCuPy offers a NumPy-like interface for GPU arrays. In this section, we perform some simple array computations on the GPU and compare the performance with CPU-based NumPy computations. This demonstrates how to fully harness the GPU power available in your environment.","metadata":{}},{"id":"f4223d0b-5a7a-47a3-a2fa-cd0a5b5fdc5a","cell_type":"code","source":"import time\n\n# CPU-based computation using NumPy\na_cpu = np.random.rand(10000000)\nstart_time = time.time()\nresult_cpu = np.sqrt(a_cpu)\ncpu_time = time.time() - start_time\nprint(f\"CPU (NumPy) computation time: {cpu_time:.5f} seconds\")\n\n# GPU-based computation using CuPy\na_gpu = cp.random.rand(10000000)\ncp.cuda.Stream.null.synchronize()  # Ensure previous operations are complete\nstart_time = time.time()\nresult_gpu = cp.sqrt(a_gpu)\ncp.cuda.Stream.null.synchronize()  # Wait for GPU computation to finish\ngpu_time = time.time() - start_time\nprint(f\"GPU (CuPy) computation time: {gpu_time:.5f} seconds\")\n\n# Verify results (convert GPU result to CPU)\nresult_gpu_cpu = cp.asnumpy(result_gpu)\nprint(\"Verification: Maximum difference between CPU and GPU results:\", np.max(np.abs(result_cpu - result_gpu_cpu)))","metadata":{},"outputs":[],"execution_count":null},{"id":"bf7a21d7-cf1c-47d4-9c9f-6e7a5d8b2a70","cell_type":"markdown","source":"### 5.4 Comparing CPU and GPU Models\n\nWe now compare the accuracy of the CPU- and GPU-based models. Even though the Iris dataset is small, this section demonstrates how similar workflows can be accelerated on larger datasets.","metadata":{}},{"id":"8f05a8e8-06ee-4de2-9c68-d7d30c18e619","cell_type":"code","source":"print(\"CPU Model Accuracy:\", accuracy)\nprint(\"GPU Model Accuracy:\", accuracy_gpu)","metadata":{},"outputs":[],"execution_count":null},{"id":"a3a36075-b2a4-4215-b59c-4cc1850b6bfa","cell_type":"markdown","source":"## 6. Feature Importance Analysis\n\nUnderstanding which features influence the model's predictions can provide insight into the data. Below, we display feature importances calculated from the CPU-based Random Forest model.","metadata":{}},{"id":"d06a8cfd-c1ce-45ea-9fce-daf1b5b7b349","cell_type":"code","source":"# Extract and visualize feature importances\nfeature_importances = pd.Series(clf.feature_importances_, index=iris.feature_names).sort_values(ascending=False)\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x=feature_importances, y=feature_importances.index)\nplt.title(\"Feature Importance in the Iris Dataset\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"b1916c7b-bdf5-4d78-8dd5-d8a4a02c61f1","cell_type":"markdown","source":"## 7. Additional Learning Resources and Next Steps\n\nFor further exploration and to continue expanding your GPU-accelerated data science skills, consider the following resources:\n\n- **Python for Data Analysis** by Wes McKinney – In-depth guidance on effective data manipulation.\n- [Pandas Documentation](https://pandas.pydata.org/docs/) – Official resource for data manipulation with Pandas.\n- [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html) – Advanced techniques for data visualization.\n- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html) – Comprehensive resource on machine learning in Python.\n- [RAPIDS Documentation](https://docs.rapids.ai/) – Detailed information on GPU-accelerated libraries such as cuDF and cuML.\n- [CuPy Documentation](https://docs.cupy.dev/en/stable/) – Learn how to perform high-speed array computations on the GPU.\n\nThese materials will help you tackle more complex projects and datasets using both CPU and GPU methodologies.","metadata":{}},{"id":"48ef08bd-9d64-4ea4-a6d1-2a5f1eaa0d4e","cell_type":"markdown","source":"## 8. Summary and Conclusions\n\nIn this notebook, we:\n- Loaded and preprocessed a classic dataset.\n- Conducted comprehensive exploratory data analysis with visualizations.\n- Developed and evaluated a Random Forest classifier on both CPU and GPU using scikit-learn and cuML.\n- Demonstrated GPU-accelerated array computations with CuPy and measured performance improvements.\n- Discussed additional resources to further develop your GPU-accelerated data science skills.\n\nThis notebook is designed as a detailed resource to bridge traditional data workflows with modern GPU acceleration techniques. By leveraging these methods, you can scale your data analysis and machine learning projects to larger, more complex datasets.","metadata":{}}]}